{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by: Daniel Seungwook Min, Doyle Group \n",
    "# Please contact mindaniel@chem.ucla.edu for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "unprocessed_train = pd.read_csv(\"datasets/unprocessed_train_set.csv\", index_col = 0)\n",
    "processed_train = pd.read_csv(\"datasets/processed_train_set.csv\", index_col = 0)\n",
    "condition_space = pd.read_csv(\"datasets/condition_space.csv\", index_col = 0)\n",
    "\n",
    "# Define input and label\n",
    "X = processed_train.drop(['z%'], axis = 1)\n",
    "y = processed_train['z%']\n",
    "\n",
    "# Convert label from z% to ∆∆G‡\n",
    "T = unprocessed_train['temperature']\n",
    "y_ddg = -1.987*(T+273.15)*np.log(y/(1-y))\n",
    "\n",
    "# Structural data\n",
    "is_PAr3_df = pd.read_csv(\"datasets/is_PAr3_all_ligands.csv\", index_col = 0).dropna()\n",
    "is_PAr3_train = condition_space['ligand_ID'].map(lambda x: int(is_PAr3_df['PAr3'][x]))\n",
    "is_PAr3_train = is_PAr3_train.rename(\"PAr3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward greedy feature selection\n",
    "from sklearn.model_selection import KFold, cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# Optional switch to run the following code. Typically this code is expensive\n",
    "toRun = False\n",
    "\n",
    "\n",
    "if toRun:\n",
    "    lolo = LeaveOneGroupOut() # cross validation split\n",
    "\n",
    "    # Find best fit using same parameters as main model\n",
    "    indices = list(lolo.split(X, y_ddg, groups = condition_space['ligand_ID']))\n",
    "    kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "    regr = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "                                    n_restarts_optimizer = 5, normalize_y = True)\n",
    "    sfs = SequentialFeatureSelector(regr, cv = indices, n_features_to_select = 'auto', \n",
    "                                    scoring = 'neg_mean_absolute_error', tol = 0.02)\n",
    "\n",
    "    sfs.fit(X, y_ddg)\n",
    "    extracted_features = X.columns[sfs.get_support()]\n",
    "    print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset of features to use. \n",
    "# Extracted from previous cell\n",
    "feats = ['reductant_n_carbons', 'reductant_n_beta_H', 'reductant_metal',\n",
    "       'ligand_nmrtens_szz_P_boltz', 'ligand_efg_amp_P_boltz',\n",
    "       'ligand_efgtens_xx_P_boltz', 'ligand_nbo_lp_P_percent_s_boltz',\n",
    "       'ligand_nbo_lp_P_occ_boltz', 'ligand_nbo_bds_e_avg_boltz',\n",
    "       'ligand_nbo_bd_occ_avg_boltz', 'ligand_nbo_bds_occ_avg_boltz',\n",
    "       'ligand_E_solv_total_boltz', 'ligand_pyr_P_boltz', 'ligand_pyr_P_delta',\n",
    "       'ligand_vbur_far_vbur_boltz', 'ligand_vbur_far_vbur_min',\n",
    "       'ligand_sterimol_B1_boltz', 'ligand_sterimol_B1_min',\n",
    "       'ligand_sterimol_B5_delta', 'ligand_sterimol_burB5_min']\n",
    "\n",
    "# Uncomment this for more modular flow. Otherwise skip previous cell (expensive)\n",
    "# feats = extracted_features\n",
    "\n",
    "X = X[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "base_model = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "                                n_restarts_optimizer = 25, normalize_y = True)\n",
    "k_folds = LeaveOneGroupOut()\n",
    "indices = k_folds.split(X, y_ddg, groups = condition_space['ligand_ID'])\n",
    "scores = cross_validate(base_model, X, y_ddg, cv = indices, \n",
    "                        scoring = 'neg_mean_absolute_error', \n",
    "                        return_train_score = True)\n",
    "                \n",
    "\n",
    "print(\"Predicting Z% for base model...\")\n",
    "print(\"Test z% MAE mean\", round(-scores['test_score'].mean(), 2), \"cal/mol\")\n",
    "print(\"Test z% MAE std\",  round(scores['test_score'].std(), 2), \"cal/mol\")\n",
    "\n",
    "print(\"Train z% MAE mean\", round(-scores['train_score'].mean(),2), \"cal/mol\")\n",
    "print(\"Train z% MAE std\",  round(scores['train_score'].std(), 2), \"cal/mol\")\n",
    "\n",
    "print(\"Number of CV Scores (ligands) used in Average: \", len(scores['test_score']))\n",
    "print(\"Features used: \", feats)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated estimates for each input data point using LOGO CV\n",
    "indices = k_folds.split(X, y_ddg, groups = condition_space['ligand_ID'])\n",
    "\n",
    "base_preds = cross_val_predict(base_model, X, y_ddg, cv = indices)\n",
    "base_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ddg_base_error = y_ddg - base_preds\n",
    "y_ddg_base_error = pd.concat([y_ddg_base_error, condition_space['ligand_ID'], \n",
    "                              is_PAr3_train], axis = 1)\n",
    "part_par3_final = y_ddg_base_error[y_ddg_base_error['PAr3'] == 1].drop(['PAr3'], axis = 1)\n",
    "grouped_par3 = part_par3_final.groupby(by = 'ligand_ID').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error by groups (PAr3 vs PR3)\n",
    "y_ddg_base_error = abs(y_ddg - base_preds)\n",
    "y_ddg_base_error = pd.concat([y_ddg_base_error, condition_space['ligand_ID'], \n",
    "                              is_PAr3_train], axis = 1)\n",
    "y_ddg_base_error = y_ddg_base_error.rename(columns = {0: \"errors\"})\n",
    "part_par3_final = y_ddg_base_error[y_ddg_base_error['PAr3'] == 1].drop(['PAr3'], axis = 1)\n",
    "part_pr3_final = y_ddg_base_error[y_ddg_base_error['PAr3'] == 0].drop(['PAr3'], axis = 1)\n",
    "\n",
    "grouped_par3 = part_par3_final.groupby(by = 'ligand_ID').mean()\n",
    "print(\"PAr3\")\n",
    "print(\"MAE: \", round(grouped_par3.mean(),2).to_string(index=False), \"cal/mol\")\n",
    "print(\"MAE STD: \", round(grouped_par3.std(),2).to_string(index=False), \"cal/mol\")\n",
    "\n",
    "grouped_pr3 = part_pr3_final.groupby(by = 'ligand_ID').mean()\n",
    "print(\"\\nPR3\")\n",
    "print(\"MAE: \", round(grouped_pr3.mean(),2).to_string(index=False), \"cal/mol\")\n",
    "print(\"MAE STD: \", round(grouped_pr3.std(),2).to_string(index=False), \"cal/mol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.scatter(y_ddg, base_preds)\n",
    "plt.xlabel(\"Experimental ∆∆G‡\")\n",
    "plt.ylabel(\"Predicted ∆∆G‡\")\n",
    "plt.title(\"∆∆G‡ Predicted vs Experimental\")\n",
    "plt.plot([-1500,1500], [-1500,1500], '--', color = 'black')\n",
    "plt.legend(['Conditions', 'Ideal line'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of base model in z%\n",
    "\n",
    "T_kelvin = T + 273.15\n",
    "term1 = np.exp(-base_preds/(1.987*T_kelvin))\n",
    "base_preds_percent = term1/(1+term1)\n",
    "\n",
    "y_percent_base_error = abs(y - base_preds_percent)\n",
    "y_percent_base_error = pd.concat([y_percent_base_error, condition_space['ligand_ID']], axis = 1)\n",
    "y_percent_base_error = y_percent_base_error.rename(columns = {0: \"errors\"})\n",
    "\n",
    "grouped_y_percent = y_percent_base_error.groupby(by = 'ligand_ID').mean()\n",
    "print(\"MAE in z%\")\n",
    "print(\"MAE: \", round(grouped_y_percent.mean()*100,2).to_string(index=False), '%')\n",
    "print(\"MAE STD: \", round(grouped_y_percent.std()*100,2).to_string(index=False), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.scatter(y, base_preds_percent)\n",
    "plt.xlabel(\"Experimental z%\")\n",
    "plt.ylabel(\"Predicted z%\")\n",
    "plt.title(\"z% Predicted vs Experimental\")\n",
    "plt.plot([0,1], [0,1], '--', color = 'black')\n",
    "plt.legend(['Conditions', 'Ideal line'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and label\n",
    "X_delta = processed_train.drop(['z%'], axis = 1)\n",
    "y_ddg_base_error = y_ddg - base_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No feature split, no training set split (nf_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward greedy feature selection: Delta models\n",
    "\n",
    "# Optional switch to run the following code. Typically this code is expensive\n",
    "toRun = False\n",
    "\n",
    "\n",
    "if toRun:\n",
    "    lolo = LeaveOneGroupOut() # cross validation split\n",
    "\n",
    "    # Find best fit using same hyperparameters as main model\n",
    "    indices = list(lolo.split(X_delta, y_ddg_base_error, \n",
    "                   groups = condition_space['ligand_ID']))\n",
    "    kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "    regr = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "                                    n_restarts_optimizer = 5, normalize_y = True)\n",
    "    sfs = SequentialFeatureSelector(regr, cv = indices, n_features_to_select = 'auto', \n",
    "                                    scoring = 'neg_mean_absolute_error', tol = 0.02)\n",
    "\n",
    "    sfs.fit(X_delta, y_ddg_base_error)\n",
    "    extracted_features = X_delta.columns[sfs.get_support()]\n",
    "    print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used in the delta model, as well as the input and output\n",
    "feats_delta_nf_nt = ['solvent_dielectric constant', 'reductant_metal',\n",
    "       'ligand_nbo_bds_e_avg_boltz', 'ligand_E_oxidation_boltz',\n",
    "       'ligand_pyr_P_delta']\n",
    "\n",
    "# Uncomment this for more modular flow. Otherwise skip previous cell (expensive)\n",
    "# feats = extracted_features\n",
    "\n",
    "X_delta_nf_nt = X_delta[feats_delta_nf_nt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated estimates for each input data point using LOGO CV\n",
    "indices = k_folds.split(X_delta_nf_nt, y_ddg_base_error, \n",
    "                        groups = condition_space['ligand_ID'])\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "delta_model_nf_nt = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True)\n",
    "error_preds_nf_nt = cross_val_predict(delta_model_nf_nt, X_delta_nf_nt, \n",
    "              y_ddg_base_error, cv = indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"r2:\", round(r2_score(y_ddg_base_error, error_preds_nf_nt),2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_ddg_base_error, error_preds_nf_nt)\n",
    "plt.xlabel(\"Actual error\")\n",
    "plt.ylabel(\"Predicted error\")\n",
    "plt.title(\"Error prediction by delta model\")\n",
    "plt.plot([-750, 750], [-750, 750], '--', color = 'black')\n",
    "\n",
    "# Arbitrary threshold to avoid small discrepency along the decision boundary to\n",
    "# be counted towards the stat (i.e. +1 cal/mol vs -1 cal/mol)\n",
    "thres = 30\n",
    "ax.axhspan(-thres, thres, color = 'orange', alpha = 0.5)\n",
    "ax.axvspan(-thres, thres, color = 'orange', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "error_anal_nf_nt = pd.DataFrame({\"actual\": y_ddg_base_error,\n",
    "                               \"predicted\": error_preds_nf_nt})\n",
    "error_anal_nf_nt = error_anal_nf_nt[abs(error_anal_nf_nt['actual']) > 30]\n",
    "error_anal_nf_nt = error_anal_nf_nt[abs(error_anal_nf_nt['predicted']) > 30]\n",
    "error_anal_nf_nt = error_anal_nf_nt > 0\n",
    "confusion_m = confusion_matrix(error_anal_nf_nt['actual'], error_anal_nf_nt['predicted'])\n",
    "ConfusionMatrixDisplay(confusion_m, display_labels = ['Over', 'Under']).plot()\n",
    "print(\"accurary:\", round((confusion_m[0][0]+confusion_m[1][1])/np.sum(confusion_m), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New error with adjusted predictions\n",
    "errors_nf_nt = abs(y_ddg - (base_preds + error_preds_nf_nt))\n",
    "errors_nf_nt_df = pd.DataFrame({\"errors\": errors_nf_nt, \n",
    "                  \"group\":condition_space[\"ligand_ID\"]})\n",
    "grouped = errors_nf_nt_df.groupby(by = 'group').mean()\n",
    "print(\"MAE: \", round(grouped.mean(), 2))\n",
    "print(\"MAE STD: \", round(grouped.std()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error by groups (PAr3 vs PR3)\n",
    "errors_nf_nt_df['PAr3'] = is_PAr3_train\n",
    "part_par3_final = errors_nf_nt_df[errors_nf_nt_df['PAr3'] == 1].drop(['PAr3'], axis = 1)\n",
    "part_pr3_final = errors_nf_nt_df[errors_nf_nt_df['PAr3'] == 0].drop(['PAr3'], axis = 1)\n",
    "\n",
    "grouped_par3 = part_par3_final.groupby(by = 'group').mean()\n",
    "print(\"PAr3\")\n",
    "print(\"MAE: \", round(grouped_par3.mean(),2).to_string(index=False))\n",
    "print(\"MAE STD: \", round(grouped_par3.std(),2).to_string(index=False))\n",
    "\n",
    "grouped_pr3 = part_pr3_final.groupby(by = 'group').mean()\n",
    "print(\"\\nPR3\")\n",
    "print(\"MAE: \", round(grouped_pr3.mean(),2).to_string(index=False))\n",
    "print(\"MAE STD: \", round(grouped_pr3.std(),2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both base and adjusted model\n",
    "adjusted = base_preds + error_preds_nf_nt\n",
    "plt.scatter(y_ddg, base_preds)\n",
    "plt.scatter(y_ddg, adjusted)\n",
    "plt.plot([-1500,1500], [-1500,1500], '--', color = 'black')\n",
    "plt.legend([\"base model\", \"delta model\"])\n",
    "plt.title(\"Predictability of base model and delta model: ∆∆G‡\")\n",
    "plt.xlabel(\"Experimental\")\n",
    "plt.ylabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of nf_nt delta model in z%\n",
    "\n",
    "T_kelvin = T + 273.15\n",
    "term1 = np.exp(-adjusted/(1.987*T_kelvin))\n",
    "nf_nt_preds_percent = term1/(1+term1)\n",
    "\n",
    "y_percent_nf_nt_error = abs(y - nf_nt_preds_percent)\n",
    "y_percent_nf_nt_error = pd.concat([y_percent_nf_nt_error, condition_space['ligand_ID']], axis = 1)\n",
    "y_percent_nf_nt_error = y_percent_nf_nt_error.rename(columns = {0: \"errors\"})\n",
    "\n",
    "grouped_y_percent_nf_nt = y_percent_nf_nt_error.groupby(by = 'ligand_ID').mean()\n",
    "print(\"MAE in z%\")\n",
    "print(\"MAE: \", round(grouped_y_percent_nf_nt.mean()*100,2).to_string(index=False), '%')\n",
    "print(\"MAE STD: \", round(grouped_y_percent_nf_nt.std()*100,2).to_string(index=False), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.scatter(y, nf_nt_preds_percent)\n",
    "plt.xlabel(\"Experimental z%\")\n",
    "plt.ylabel(\"Predicted z%\")\n",
    "plt.title(\"z% Predicted vs Experimental\")\n",
    "plt.plot([0,1], [0,1], '--', color = 'black')\n",
    "plt.legend(['Conditions', 'Ideal line'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature split, training set split (f_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine relevant data into one to avoid possibility of data leakage\n",
    "combined = pd.concat([processed_train, \n",
    "           pd.DataFrame(base_preds, index = processed_train.index, columns = ['base_preds']),\n",
    "           pd.DataFrame(y_ddg, index = processed_train.index, columns = ['y_ddg']), \n",
    "           condition_space['ligand_ID'].to_frame()],\n",
    "           axis = 1)\n",
    "\n",
    "# Import structural data, add them into combined df\n",
    "is_PAr3_df = pd.read_csv(\"datasets/is_PAr3_all_ligands.csv\", index_col = 0).dropna()\n",
    "combined['PAr3'] = combined['ligand_ID'].map(lambda x: int(is_PAr3_df['PAr3'][x]))\n",
    "part_par3 = combined[combined['PAr3'] == 1]\n",
    "part_pr3 = combined[combined['PAr3'] == 0]\n",
    "\n",
    "# Sanity check: Check no overlapping datapoints between the two partitions\n",
    "assert(len(set(part_par3.index).intersection(set(part_pr3.index))) == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta model for Triaryl monophosphine partition (PAr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and label\n",
    "X_par3 = part_par3.drop(['z%', 'base_preds', 'y_ddg', 'ligand_ID', 'PAr3'], axis = 1)\n",
    "y_ddg_base_error_par3 = part_par3['y_ddg'] - part_par3['base_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward greedy feature selection: Delta models - PAr3 partition\n",
    "\n",
    "# Optional switch to run the following code. Typically this code is expensive\n",
    "toRun = False\n",
    "\n",
    "\n",
    "if toRun:\n",
    "    lolo = LeaveOneGroupOut() # cross validation split\n",
    "    \n",
    "    # Find best fit using same hyperparameters as main model\n",
    "    indices = list(lolo.split(X_par3, y_ddg_base_error_par3, \n",
    "                   groups = part_par3['ligand_ID']))\n",
    "    kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "    regr = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "                                    n_restarts_optimizer = 5, normalize_y = True)\n",
    "    sfs = SequentialFeatureSelector(regr, cv = indices, n_features_to_select = 'auto', \n",
    "                                    scoring = 'neg_mean_absolute_error', tol = 0.01)\n",
    "\n",
    "    sfs.fit(X_par3, y_ddg_base_error_par3)\n",
    "    extracted_features = X_par3.columns[sfs.get_support()]\n",
    "    print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used in the delta model, as well as the input and output\n",
    "feats_delta_par3 = ['reductant_n_carbons', 'reductant_metal',\n",
    "       'ligand_nbo_lp_P_percent_s_boltz', 'ligand_nbo_bd_e_avg_boltz',\n",
    "       'ligand_vbur_ovbur_min_vburminconf']\n",
    "\n",
    "# Uncomment this for more modular flow. Otherwise skip previous cell (expensive)\n",
    "# feats = extracted_features\n",
    "\n",
    "X_delta_par3 = X_par3[feats_delta_par3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta model for Non-Triaryl monophosphine partition (PR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and label\n",
    "X_pr3 = part_pr3.drop(['z%', 'base_preds', 'y_ddg', 'ligand_ID', 'PAr3'], axis = 1)\n",
    "y_ddg_base_error_pr3 = part_pr3['y_ddg'] - part_pr3['base_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not split features, not split training set\n",
    "# Cross-validated estimates for each input data point using LOGO CV\n",
    "indices = k_folds.split(X_delta_par3, y_ddg_base_error_par3, \n",
    "                        groups = part_par3['ligand_ID'])\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "delta_model_par3 = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True)\n",
    "error_preds_par3 = cross_val_predict(delta_model_par3, X_delta_par3, \n",
    "              y_ddg_base_error_par3, cv = indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward greedy feature selection: Delta models - PR3 partition\n",
    "\n",
    "# Optional switch to run the following code. Typically this code is expensive\n",
    "toRun = False\n",
    "\n",
    "\n",
    "\n",
    "if toRun:\n",
    "    X_pr3 = part_pr3.drop(['z%', 'base_preds', 'y_ddg', 'ligand_ID', 'PAr3'], axis = 1)\n",
    "    y_ddg_base_error_pr3 = part_pr3['y_ddg'] - part_pr3['base_preds']\n",
    "    lolo = LeaveOneGroupOut() # cross validation split\n",
    "    \n",
    "    # Find best fit using same hyperparameters as main model\n",
    "    indices = list(lolo.split(X_pr3, y_ddg_base_error_pr3, \n",
    "                   groups = part_pr3['ligand_ID']))\n",
    "    kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "    regr = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "                                    n_restarts_optimizer = 5, normalize_y = True)\n",
    "    sfs = SequentialFeatureSelector(regr, cv = indices, n_features_to_select = 'auto', \n",
    "                                    scoring = 'neg_mean_absolute_error', tol = 0.02)\n",
    "\n",
    "    sfs.fit(X_pr3, y_ddg_base_error_pr3)\n",
    "    extracted_features = X_pr3.columns[sfs.get_support()]\n",
    "    print(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used in the delta model, as well as the input and output\n",
    "feats_delta_pr3 =  ['ligand_E_solv_cds_boltz','ligand_qpole_amp_boltz','reductant_n_beta_H']\n",
    "\n",
    "# Uncomment this for more modular flow. Otherwise skip previous cell (expensive)\n",
    "# feats = extracted_features\n",
    "\n",
    "X_delta_pr3 = X_pr3[feats_delta_pr3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not split features, not split training set\n",
    "# Cross-validated estimates for each input data point using LOGO CV\n",
    "indices = k_folds.split(X_delta_pr3, y_ddg_base_error_pr3, \n",
    "                        groups = part_pr3['ligand_ID'])\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "delta_model_pr3 = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True)\n",
    "error_preds_pr3 = cross_val_predict(delta_model_pr3, X_delta_pr3, \n",
    "              y_ddg_base_error_pr3, cv = indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both delta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from both par3 and pr3 partitions\n",
    "error_preds_f_t = pd.concat(\n",
    "    [pd.DataFrame(error_preds_par3, index = part_par3.index, columns = ['preds_f_t']),\n",
    "     pd.DataFrame(error_preds_pr3, index = part_pr3.index, columns = ['preds_f_t'])])\n",
    "error_preds_f_t = error_preds_f_t.reindex(processed_train.index).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"r2:\", round(r2_score(y_ddg_base_error, error_preds_f_t),2))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_ddg_base_error, error_preds_f_t)\n",
    "plt.xlabel(\"Actual error\")\n",
    "plt.ylabel(\"Predicted error\")\n",
    "plt.title(\"Error prediction by delta model\")\n",
    "plt.plot([-750, 750], [-750, 750], '--', color = 'black')\n",
    "\n",
    "thres = 30\n",
    "ax.axhspan(-thres, thres, color = 'orange', alpha = 0.5)\n",
    "ax.axvspan(-thres, thres, color = 'orange', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "error_anal_f_t = pd.DataFrame({\"actual\": y_ddg_base_error,\n",
    "                               \"predicted\": error_preds_f_t})\n",
    "error_anal_f_t = error_anal_f_t[abs(error_anal_f_t['actual']) > 30]\n",
    "error_anal_f_t = error_anal_f_t[abs(error_anal_f_t['predicted']) > 30]\n",
    "error_anal_f_t = error_anal_f_t > 0\n",
    "confusion_m = confusion_matrix(error_anal_f_t['actual'], error_anal_f_t['predicted'])\n",
    "ConfusionMatrixDisplay(confusion_m, display_labels = ['Over', 'Under']).plot()\n",
    "print(\"accurary:\", round((confusion_m[0][0]+confusion_m[1][1])/np.sum(confusion_m), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New error with adjusted predictions\n",
    "errors_f_t = abs(y_ddg - (base_preds + error_preds_f_t))\n",
    "errors_f_t_df = pd.DataFrame({\"errors\": errors_f_t, \n",
    "                  \"group\":condition_space[\"ligand_ID\"]})\n",
    "grouped = errors_f_t_df.groupby(by = 'group').mean()\n",
    "print(\"MAE: \", round(grouped.mean(), 2))\n",
    "print(\"MAE STD: \", round(grouped.std()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error by groups (PAr3 vs PR3)\n",
    "errors_f_t_df['PAr3'] = combined['PAr3']\n",
    "part_par3_final = errors_f_t_df[errors_f_t_df['PAr3'] == 1].drop(['PAr3'], axis = 1)\n",
    "part_pr3_final = errors_f_t_df[errors_f_t_df['PAr3'] == 0].drop(['PAr3'], axis = 1)\n",
    "\n",
    "grouped_par3 = part_par3_final.groupby(by = 'group').mean()\n",
    "print(\"PAr3\")\n",
    "print(\"MAE: \", round(grouped_par3.mean(),2).to_string(index=False))\n",
    "print(\"MAE STD: \", round(grouped_par3.std(),2).to_string(index=False))\n",
    "\n",
    "grouped_pr3 = part_pr3_final.groupby(by = 'group').mean()\n",
    "print(\"\\nPR3\")\n",
    "print(\"MAE: \", round(grouped_pr3.mean(),2).to_string(index=False))\n",
    "print(\"MAE STD: \", round(grouped_pr3.std(),2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_ddg, base_preds)\n",
    "plt.scatter(y_ddg, base_preds + error_preds_nf_nt)\n",
    "plt.scatter(y_ddg, base_preds + error_preds_f_t)\n",
    "# plt.scatter(part_par3['y_ddg'], error_preds_par3)\n",
    "plt.plot([-1500, 1500], [-1500, 1500], '--', color = 'black',)\n",
    "plt.legend(['base only', '∆nf_nt', '∆f_t', 'ideal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of f_t delta model in z%\n",
    "adjusted_preds_f_t = base_preds + error_preds_f_t\n",
    "\n",
    "T_kelvin = T + 273.15\n",
    "term1 = np.exp(-adjusted_preds_f_t/(1.987*T_kelvin))\n",
    "f_t_preds_percent = term1/(1+term1)\n",
    "\n",
    "y_percent_f_t_error = abs(y - f_t_preds_percent)\n",
    "y_percent_f_t_error = pd.concat([y_percent_f_t_error, condition_space['ligand_ID']], axis = 1)\n",
    "y_percent_f_t_error = y_percent_f_t_error.rename(columns = {0: \"errors\"})\n",
    "\n",
    "grouped_y_percent_f_t = y_percent_f_t_error.groupby(by = 'ligand_ID').mean()\n",
    "print(\"MAE in z%\")\n",
    "print(\"MAE: \", round(grouped_y_percent_f_t.mean()*100,2).to_string(index=False), '%')\n",
    "print(\"MAE STD: \", round(grouped_y_percent_f_t.std()*100,2).to_string(index=False), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.scatter(y, f_t_preds_percent)\n",
    "plt.xlabel(\"Experimental z%\")\n",
    "plt.ylabel(\"Predicted z%\")\n",
    "plt.title(\"z% Predicted vs Experimental\")\n",
    "plt.plot([0,1], [0,1], '--', color = 'black')\n",
    "plt.legend(['Conditions', 'Ideal line'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"datasets/test_set.csv\", index_col = 0)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize test set based on scaling factors of train set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "OHE_cols = ['reductant_metal']\n",
    "OHE = test_set[OHE_cols]\n",
    "nonOHE = test_set.drop(OHE_cols, axis = 1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(unprocessed_train[nonOHE.columns])\n",
    "test_set = pd.DataFrame(scaler.transform(test_set.drop(OHE_cols, axis = 1)), \n",
    "                    index = test_set.index, columns = nonOHE.columns)\n",
    "for col in OHE_cols:\n",
    "    test_set[col] = OHE[col]\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model trained on whole training set\n",
    "\n",
    "# Base Model\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "base_test_x = processed_train[feats]\n",
    "base_test_y = y_ddg\n",
    "\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "base_model = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True).fit(base_test_x, base_test_y)\n",
    "print(\"Score:\", round(base_model.score(base_test_x, base_test_y),2))\n",
    "                \n",
    "base_model_whole_preds = base_model.predict(base_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make base model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into partition size suitable for local computer's RAM capacity.\n",
    "# Adjust according to RAM. Maximum size is 5860800\n",
    "partition_size = 3000000\n",
    "first_half_base = base_model.predict(test_set[feats][:partition_size])\n",
    "second_half_base = base_model.predict(test_set[feats][partition_size:])\n",
    "kraken_base_preds = np.concatenate([first_half_base, second_half_base])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make delta model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one hot encoded PAr3 identifier to split the test set into PAr3 and PR3 partitions\n",
    "kraken_pre_feats = pd.read_csv(\"datasets/predicted_delta_model_sorted.csv\", \n",
    "            index_col = 0).drop(['base_preds', 'delta_preds', 'final_preds', \n",
    "                                 'predicted_z'], axis = 1)\n",
    "PAr3_test_set = kraken_pre_feats['ligands'].map(lambda x: is_PAr3_df['PAr3'][x])\n",
    "test_set['PAr3'] = PAr3_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta model predictions PR3\n",
    "test_set_delta_pr3 = test_set[test_set['PAr3'] == 0]\n",
    "\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "delta_model_pr3 = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True).fit(X_delta_pr3, y_ddg_base_error_pr3)\n",
    "print(\"Score:\", round(delta_model_pr3.score(X_delta_pr3, y_ddg_base_error_pr3), 2))\n",
    "                \n",
    "delta_model_pr3_whole_preds = delta_model_pr3.predict(X_delta_pr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_pr3_test_set_preds_first_half = delta_model_pr3.predict(test_set_delta_pr3[feats_delta_pr3])[:partition_size]\n",
    "delta_pr3_test_set_preds_second_half = delta_model_pr3.predict(test_set_delta_pr3[feats_delta_pr3])[partition_size:]\n",
    "\n",
    "delta_pr3_test_set_preds = np.concatenate([delta_pr3_test_set_preds_first_half, delta_pr3_test_set_preds_second_half])\n",
    "delta_pr3_test_set_preds = pd.DataFrame(delta_pr3_test_set_preds, index = test_set_delta_pr3.index, columns = ['delta_preds'])\n",
    "delta_pr3_test_set_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta model predictions par3\n",
    "test_set_delta_par3 = test_set[test_set['PAr3'] == 1]\n",
    "\n",
    "kernel = Matern(length_scale = 1.5, nu = 2.5)\n",
    "delta_model_par3 = GaussianProcessRegressor(kernel = kernel, alpha = 0.5, \n",
    "              n_restarts_optimizer = 25, normalize_y = True).fit(X_delta_par3, y_ddg_base_error_par3)\n",
    "print(\"Score:\", round(delta_model_par3.score(X_delta_par3, y_ddg_base_error_par3), 2))\n",
    "                \n",
    "delta_model_par3_whole_preds = delta_model_par3.predict(X_delta_par3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_par3_test_set_preds = delta_model_par3.predict(test_set_delta_par3[feats_delta_par3])\n",
    "delta_par3_test_set_preds = pd.DataFrame(delta_par3_test_set_preds, index = test_set_delta_par3.index, columns = ['delta_preds'])\n",
    "delta_par3_test_set_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all predictions\n",
    "base_test_preds_all = pd.DataFrame(kraken_base_preds, index = test_set.index, columns = ['base_preds'])\n",
    "delta_test_preds_all = pd.concat([delta_pr3_test_set_preds, delta_par3_test_set_preds]).reindex(test_set.index)\n",
    "final_preds = base_test_preds_all['base_preds'] + delta_test_preds_all['delta_preds']\n",
    "final_preds = pd.DataFrame(final_preds, index = base_test_preds_all.index, columns = ['final_preds'])\n",
    "\n",
    "condition_cols = ['reductant', 'reductant_C', 'temperature', 'catalyst_C', 'cat2lig',\n",
    "       'solvent', 'concentration', 'ligands']\n",
    "final_preds_with_conditions = pd.concat([kraken_pre_feats[condition_cols], final_preds], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted ∆∆G‡ to z%\n",
    "T_test = final_preds_with_conditions['temperature'] + 273.15\n",
    "ddg_test = final_preds_with_conditions['final_preds']\n",
    "R_test = 1.987# R is constant but named with _test for avoid any other variable collisions\n",
    "val = np.exp(-ddg_test/(R_test*T_test))\n",
    "z_test = val/(1+val)\n",
    "final_preds_with_conditions['final_preds_z'] = z_test\n",
    "\n",
    "# Add smiles string of the ligand\n",
    "id_to_smiles = pd.read_csv(\"datasets/kraken_structural_OHE.csv\", index_col = 1)['CAN SMILES']\n",
    "smiles = final_preds_with_conditions['ligands'].map(lambda x: id_to_smiles.loc[x])\n",
    "final_preds_with_conditions['smiles'] = smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract condition with highest predicted selectivity for each ligand\n",
    "\n",
    "top_zs = final_preds_with_conditions.groupby('ligands').max().sort_values(by = 'final_preds_z', ascending = False)\n",
    "# top_zs.to_csv(\"datasets/top_z_predicted.csv\")\n",
    "\n",
    "top_es = final_preds_with_conditions.groupby('ligands').min().sort_values(by = 'final_preds_z', ascending = True)\n",
    "# top_es.to_csv(\"datasets/top_e_predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe top predictions\n",
    "top_zs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe top predictions\n",
    "top_es.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
